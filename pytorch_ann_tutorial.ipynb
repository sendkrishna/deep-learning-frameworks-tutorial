{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Artificial Neural Networks (ANNs)\n",
    "\n",
    "## Welcome to PyTorch!\n",
    "\n",
    "This comprehensive tutorial will teach you how to build, train, and evaluate Artificial Neural Networks using **PyTorch**.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand PyTorch tensors and basic operations\n",
    "- Build neural network models using `nn.Module`\n",
    "- Implement custom training and evaluation loops\n",
    "- Use DataLoader for efficient data handling\n",
    "- Train models for classification and regression tasks\n",
    "- Apply regularization and optimization techniques\n",
    "- Save and load trained models\n",
    "\n",
    "### Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of basic machine learning concepts\n",
    "- NumPy fundamentals (helpful)\n",
    "\n",
    "Let's dive in! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Introduction & Setup\n",
    "\n",
    "First, let's import libraries and check our PyTorch installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Core libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \n",
    "                     'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"\\n‚úì PyTorch is ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding PyTorch\n",
    "\n",
    "**PyTorch** is a deep learning framework developed by Facebook (Meta). Key features:\n",
    "\n",
    "- üéØ **Dynamic computation graphs** - Build graphs on-the-fly\n",
    "- üî• **Pythonic** - Feels like native Python\n",
    "- üöÄ **GPU acceleration** - Easy to move models to GPU\n",
    "- üìä **Autograd** - Automatic differentiation\n",
    "- üèóÔ∏è **Flexible** - Great for research and production\n",
    "\n",
    "**PyTorch vs Keras:**\n",
    "- PyTorch: More explicit, lower-level control\n",
    "- Keras: More abstract, easier for beginners\n",
    "- Both are excellent! Choose based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick device information\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "elif device.type == 'mps':\n",
    "    print(\"Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    print(\"Using CPU - GPU not available\")\n",
    "    print(\"üí° CPU is fine for learning! All code will work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Tensor Basics\n",
    "\n",
    "**Tensors** are the fundamental data structure in PyTorch (like NumPy arrays but with GPU support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors - different methods\n",
    "\n",
    "# From Python list\n",
    "tensor_from_list = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"From list:\")\n",
    "print(tensor_from_list)\n",
    "\n",
    "# Zeros and ones\n",
    "zeros = torch.zeros(2, 3)\n",
    "ones = torch.ones(2, 3)\n",
    "print(\"\\nZeros:\\n\", zeros)\n",
    "print(\"\\nOnes:\\n\", ones)\n",
    "\n",
    "# Random tensors\n",
    "rand_tensor = torch.rand(2, 3)  # Uniform [0, 1)\n",
    "randn_tensor = torch.randn(2, 3)  # Normal distribution\n",
    "print(\"\\nRandom (uniform):\\n\", rand_tensor)\n",
    "print(\"\\nRandom (normal):\\n\", randn_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor attributes\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "print(f\"Tensor: \\n{x}\")\n",
    "print(f\"\\nShape: {x.shape}\")\n",
    "print(f\"Size: {x.size()}\")\n",
    "print(f\"Dtype: {x.dtype}\")\n",
    "print(f\"Device: {x.device}\")\n",
    "print(f\"Requires grad: {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor operations\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"\\nOperations:\")\n",
    "print(\"a + b =\", a + b)\n",
    "print(\"a * b =\", a * b)  # Element-wise\n",
    "print(\"a @ b =\", a @ b)  # Dot product\n",
    "print(\"a.sum() =\", a.sum())\n",
    "print(\"a.mean() =\", a.mean())\n",
    "print(\"a.max() =\", a.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping and indexing\n",
    "x = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
    "print(\"Original:\", x)\n",
    "\n",
    "# Reshape to 3x4\n",
    "x_reshaped = x.view(3, 4)\n",
    "print(\"\\nReshaped (3x4):\\n\", x_reshaped)\n",
    "\n",
    "# Indexing\n",
    "print(\"\\nFirst row:\", x_reshaped[0])\n",
    "print(\"First column:\", x_reshaped[:, 0])\n",
    "print(\"Element [1, 2]:\", x_reshaped[1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Tensors\n",
    "\n",
    "One of PyTorch's strengths is easy GPU acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor on CPU\n",
    "x_cpu = torch.randn(3, 3)\n",
    "print(f\"CPU tensor device: {x_cpu.device}\")\n",
    "\n",
    "# Move to GPU (if available)\n",
    "x_gpu = x_cpu.to(device)\n",
    "print(f\"GPU tensor device: {x_gpu.device}\")\n",
    "\n",
    "# Or create directly on device\n",
    "y = torch.ones(3, 3, device=device)\n",
    "print(f\"Direct creation device: {y.device}\")\n",
    "\n",
    "print(\"\\nüí° Use .to(device) to move tensors and models to GPU/CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy to PyTorch\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(\"NumPy array:\")\n",
    "print(np_array)\n",
    "print(\"\\nPyTorch tensor:\")\n",
    "print(torch_tensor)\n",
    "\n",
    "# PyTorch to NumPy (must be on CPU)\n",
    "tensor = torch.randn(2, 3)\n",
    "numpy_array = tensor.numpy()\n",
    "print(\"\\nTensor:\")\n",
    "print(tensor)\n",
    "print(\"\\nAs NumPy:\")\n",
    "print(numpy_array)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Warning: NumPy arrays and tensors share memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd - Automatic Differentiation\n",
    "\n",
    "PyTorch's **autograd** automatically computes gradients for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient tracking\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"requires_grad: {x.requires_grad}\")\n",
    "\n",
    "# Compute y = x^2 + 3\n",
    "y = x**2 + 3\n",
    "print(f\"\\ny = {y}\")\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()  # dy/dx = 2x\n",
    "print(f\"\\ndy/dx at x=2: {x.grad}\")\n",
    "print(f\"Expected (2 * 2 = 4): 4\")\n",
    "\n",
    "print(\"\\nüí° Autograd is what makes PyTorch 'auto'-matic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Data Handling\n",
    "\n",
    "PyTorch provides `Dataset` and `DataLoader` for efficient data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformation: Convert to tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load training data\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Download and load test data\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(\"\\n‚úì Fashion-MNIST loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Fashion-MNIST Samples', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = train_dataset[i]\n",
    "    # Convert from [C, H, W] to [H, W] for grayscale\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'{class_names[label]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader - Batching and Shuffling\n",
    "\n",
    "`DataLoader` handles batching, shuffling, and parallel loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Shuffle training data\n",
    "    num_workers=0  # 0 for single-process (safer for notebooks)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # Don't shuffle test data\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch images shape: {images.shape}\")  # [batch_size, channels, height, width]\n",
    "print(f\"Batch labels shape: {labels.shape}\")  # [batch_size]\n",
    "print(f\"\\nImage dtype: {images.dtype}\")\n",
    "print(f\"Label dtype: {labels.dtype}\")\n",
    "print(f\"\\nImage range: [{images.min():.2f}, {images.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset for binary classification\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification: Setosa (0) vs Not Setosa (1)\n",
    "y_binary = (y != 0).astype(np.int64)\n",
    "\n",
    "# Split data\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train_iris = scaler.fit_transform(X_train_iris)\n",
    "X_test_iris = scaler.transform(X_test_iris)\n",
    "\n",
    "print(f\"Iris dataset loaded\")\n",
    "print(f\"Training samples: {len(X_train_iris)}\")\n",
    "print(f\"Test samples: {len(X_test_iris)}\")\n",
    "print(f\"Features: {X_train_iris.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors and create TensorDataset\n",
    "X_train_tensor = torch.FloatTensor(X_train_iris)\n",
    "y_train_tensor = torch.FloatTensor(y_train_iris).unsqueeze(1)  # Add dimension\n",
    "X_test_tensor = torch.FloatTensor(X_test_iris)\n",
    "y_test_tensor = torch.FloatTensor(y_test_iris).unsqueeze(1)\n",
    "\n",
    "# Create datasets\n",
    "iris_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "iris_test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "iris_train_loader = DataLoader(iris_train_dataset, batch_size=16, shuffle=True)\n",
    "iris_test_loader = DataLoader(iris_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"‚úì Iris DataLoaders created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Building Your First Model\n",
    "\n",
    "In PyTorch, we build models by subclassing `nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding nn.Module\n",
    "\n",
    "Every PyTorch model inherits from `nn.Module` and must implement:\n",
    "1. `__init__`: Define layers\n",
    "2. `forward`: Define forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple binary classifier\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, 16)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(16, 8)           # Second hidden layer\n",
    "        self.fc3 = nn.Linear(8, 1)            # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid for binary classification\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model_binary = BinaryClassifier(input_size=4)\n",
    "model_binary = model_binary.to(device)\n",
    "\n",
    "print(model_binary)\n",
    "print(f\"\\n‚úì Model created and moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_binary.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_binary.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for name, param in model_binary.named_parameters():\n",
    "    print(f\"  {name}: {param.shape} = {param.numel():,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input\n",
    "dummy_input = torch.randn(1, 4).to(device)  # 1 sample, 4 features\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output = model_binary(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output value: {output.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model (simpler for linear architectures)\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(4, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "print(model_sequential)\n",
    "print(\"\\nüí° nn.Sequential is simpler but less flexible than nn.Module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Loss Functions & Optimizers\n",
    "\n",
    "Now we need to define how to optimize our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "Choose based on your task:\n",
    "\n",
    "| Task | Loss Function |\n",
    "|------|---------------|\n",
    "| Binary Classification | `nn.BCELoss()` or `nn.BCEWithLogitsLoss()` |\n",
    "| Multi-class Classification | `nn.CrossEntropyLoss()` |\n",
    "| Regression | `nn.MSELoss()` or `nn.L1Loss()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification loss\n",
    "criterion_binary = nn.BCELoss()  # Binary Cross Entropy\n",
    "\n",
    "# Multi-class classification loss\n",
    "criterion_multiclass = nn.CrossEntropyLoss()\n",
    "\n",
    "# Regression loss\n",
    "criterion_regression = nn.MSELoss()  # Mean Squared Error\n",
    "\n",
    "print(\"Loss functions defined:\")\n",
    "print(f\"  Binary: {criterion_binary}\")\n",
    "print(f\"  Multi-class: {criterion_multiclass}\")\n",
    "print(f\"  Regression: {criterion_regression}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "Optimizers update model parameters to minimize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common optimizers\n",
    "\n",
    "# Adam (most popular)\n",
    "optimizer_adam = optim.Adam(model_binary.parameters(), lr=0.001)\n",
    "\n",
    "# SGD (classic)\n",
    "optimizer_sgd = optim.SGD(model_binary.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# RMSprop\n",
    "optimizer_rmsprop = optim.RMSprop(model_binary.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Optimizers:\")\n",
    "print(f\"  Adam: {optimizer_adam}\")\n",
    "print(f\"  SGD: {optimizer_sgd}\")\n",
    "print(f\"  RMSprop: {optimizer_rmsprop}\")\n",
    "\n",
    "print(\"\\nüí° For beginners, use Adam with lr=0.001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different learning rates\n",
    "lr_values = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "print(\"Learning rate guidelines:\")\n",
    "for lr in lr_values:\n",
    "    if lr <= 0.001:\n",
    "        comment = \"Safe, might be slow\"\n",
    "    elif lr <= 0.01:\n",
    "        comment = \"Good starting point\"\n",
    "    else:\n",
    "        comment = \"Might be too high, check for instability\"\n",
    "    print(f\"  lr={lr}: {comment}\")\n",
    "\n",
    "print(\"\\nüí° Start with 0.001 and adjust if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: The Training Loop\n",
    "\n",
    "Unlike Keras, PyTorch requires you to write the training loop manually. This gives more control!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Structure\n",
    "\n",
    "A typical training loop:\n",
    "1. Set model to training mode\n",
    "2. For each batch:\n",
    "   - Zero gradients\n",
    "   - Forward pass\n",
    "   - Compute loss\n",
    "   - Backward pass\n",
    "   - Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for binary classification\n",
    "model_binary = BinaryClassifier(input_size=4).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_binary.parameters(), lr=0.001)\n",
    "\n",
    "print(\"‚úì Model, loss, and optimizer ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()  # Set to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Move to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úì Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úì Validation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Training binary classifier on Iris dataset...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model_binary, iris_train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(\n",
    "        model_binary, iris_test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "ax1.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Binary Classifier Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(train_accs, label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(val_accs, label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Binary Classifier Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Each Step\n",
    "\n",
    "**`optimizer.zero_grad()`**: Clear old gradients (they accumulate by default)\n",
    "\n",
    "**`loss.backward()`**: Compute gradients via backpropagation\n",
    "\n",
    "**`optimizer.step()`**: Update parameters using gradients\n",
    "\n",
    "**`model.train()` vs `model.eval()`**: Affects Dropout and BatchNorm behavior\n",
    "\n",
    "**`torch.no_grad()`**: Disables gradient computation (saves memory during evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Model Evaluation\n",
    "\n",
    "Let's properly evaluate our binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model_binary.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in iris_test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_binary(inputs)\n",
    "        predictions = (outputs > 0.5).float().cpu()\n",
    "        \n",
    "        all_predictions.extend(predictions.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "print(f\"Test Accuracy: {(all_predictions == all_labels).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix - Binary Classification', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions,\n",
    "                          target_names=['Setosa', 'Not Setosa']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sample prediction\n",
    "model_binary.eval()\n",
    "sample = X_test_tensor[0:1].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model_binary(sample)\n",
    "    probability = output.item()\n",
    "    prediction = int(probability > 0.5)\n",
    "\n",
    "print(f\"Sample features: {sample.cpu().numpy()[0]}\")\n",
    "print(f\"\\nPredicted probability: {probability:.4f}\")\n",
    "print(f\"Predicted class: {prediction} ({'Not Setosa' if prediction else 'Setosa'})\")\n",
    "print(f\"Actual class: {int(y_test_tensor[0].item())} ({'Not Setosa' if int(y_test_tensor[0].item()) else 'Setosa'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Common Layers & Techniques\n",
    "\n",
    "Let's explore important layers and regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# ReLU\n",
    "relu_output = F.relu(x_tensor).numpy()\n",
    "axes[0, 0].plot(x, relu_output, linewidth=2)\n",
    "axes[0, 0].set_title('ReLU', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid_output = torch.sigmoid(x_tensor).numpy()\n",
    "axes[0, 1].plot(x, sigmoid_output, linewidth=2)\n",
    "axes[0, 1].set_title('Sigmoid', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=0.5, color='r', linewidth=0.5, linestyle='--')\n",
    "\n",
    "# Tanh\n",
    "tanh_output = torch.tanh(x_tensor).numpy()\n",
    "axes[1, 0].plot(x, tanh_output, linewidth=2)\n",
    "axes[1, 0].set_title('Tanh', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "\n",
    "# LeakyReLU\n",
    "leaky_relu_output = F.leaky_relu(x_tensor, negative_slope=0.01).numpy()\n",
    "axes[1, 1].plot(x, leaky_relu_output, linewidth=2)\n",
    "axes[1, 1].set_title('LeakyReLU', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° ReLU is the default choice for hidden layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with Dropout\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ModelWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Drop 30% of neurons\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Only active during training\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model_dropout = ModelWithDropout(784, 256, 10)\n",
    "print(model_dropout)\n",
    "print(\"\\nüí° Dropout is automatically disabled in eval() mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with Batch Normalization\n",
    "class ModelWithBatchNorm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ModelWithBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Normalize\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model_bn = ModelWithBatchNorm(784, 256, 10)\n",
    "print(model_bn)\n",
    "print(\"\\nüí° BatchNorm normalizes activations, making training faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced model combining multiple techniques\n",
    "class AdvancedModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(AdvancedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "advanced_model = AdvancedModel(784, 10)\n",
    "print(advanced_model)\n",
    "print(\"\\n‚úì This model combines BatchNorm, Dropout, and ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Complete Project - Multi-Class Classification\n",
    "\n",
    "Let's build a complete Fashion-MNIST classifier from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Fashion-MNIST classifier\n",
    "class FashionMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 10)  # 10 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x  # No softmax (CrossEntropyLoss includes it)\n",
    "\n",
    "# Create model\n",
    "fashion_model = FashionMNISTClassifier().to(device)\n",
    "print(fashion_model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in fashion_model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "criterion_fashion = nn.CrossEntropyLoss()\n",
    "optimizer_fashion = optim.Adam(fashion_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"‚úì Ready for training!\")\n",
    "print(f\"  Loss: CrossEntropyLoss\")\n",
    "print(f\"  Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions for multi-class classification\n",
    "def train_epoch_multiclass(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def validate_multiclass(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "print(\"‚úì Training functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 15\n",
    "train_losses_fashion = []\n",
    "val_losses_fashion = []\n",
    "train_accs_fashion = []\n",
    "val_accs_fashion = []\n",
    "\n",
    "print(\"Training Fashion-MNIST classifier...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch_multiclass(\n",
    "        fashion_model, train_loader, criterion_fashion, optimizer_fashion, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc = validate_multiclass(\n",
    "        fashion_model, test_loader, criterion_fashion, device\n",
    "    )\n",
    "    \n",
    "    train_losses_fashion.append(train_loss)\n",
    "    val_losses_fashion.append(val_loss)\n",
    "    train_accs_fashion.append(train_acc)\n",
    "    val_accs_fashion.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses_fashion, label='Training Loss', linewidth=2)\n",
    "ax1.plot(val_losses_fashion, label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Fashion-MNIST Model Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_accs_fashion, label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(val_accs_fashion, label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Fashion-MNIST Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {val_accs_fashion[-1]:.4f} ({val_accs_fashion[-1]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all predictions for analysis\n",
    "fashion_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = fashion_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(\"‚úì Predictions collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.suptitle('Fashion-MNIST Predictions', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = test_dataset[i]\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    \n",
    "    color = 'green' if all_preds[i] == all_labels[i] else 'red'\n",
    "    ax.set_title(f'Pred: {class_names[all_preds[i]]}\\nTrue: {class_names[all_labels[i]]}',\n",
    "                color=color, fontsize=9, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Fashion-MNIST Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Regression with ANNs\n",
    "\n",
    "Now let's tackle regression - predicting continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "\n",
    "print(\"California Housing Dataset\")\n",
    "print(f\"Samples: {X_housing.shape[0]}\")\n",
    "print(f\"Features: {X_housing.shape[1]}\")\n",
    "print(f\"\\nFeatures: {housing.feature_names}\")\n",
    "print(f\"\\nTarget: Median house value (in $100k)\")\n",
    "print(f\"Range: ${y_housing.min()*100000:.0f} - ${y_housing.max()*100000:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_h = StandardScaler()\n",
    "X_train_h = scaler_h.fit_transform(X_train_h)\n",
    "X_test_h = scaler_h.transform(X_test_h)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_h_tensor = torch.FloatTensor(X_train_h)\n",
    "y_train_h_tensor = torch.FloatTensor(y_train_h).unsqueeze(1)\n",
    "X_test_h_tensor = torch.FloatTensor(X_test_h)\n",
    "y_test_h_tensor = torch.FloatTensor(y_test_h).unsqueeze(1)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset_h = TensorDataset(X_train_h_tensor, y_train_h_tensor)\n",
    "test_dataset_h = TensorDataset(X_test_h_tensor, y_test_h_tensor)\n",
    "\n",
    "train_loader_h = DataLoader(train_dataset_h, batch_size=32, shuffle=True)\n",
    "test_loader_h = DataLoader(test_dataset_h, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"‚úì Data prepared for regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)  # Single output for regression\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # No activation for regression\n",
    "        return x\n",
    "\n",
    "regression_model = RegressionModel(input_size=8).to(device)\n",
    "print(regression_model)\n",
    "\n",
    "print(\"\\nüí° Key differences from classification:\")\n",
    "print(\"  - 1 output neuron (not multiple classes)\")\n",
    "print(\"  - No activation on output (linear)\")\n",
    "print(\"  - MSE loss (not CrossEntropy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for regression\n",
    "criterion_reg = nn.MSELoss()\n",
    "optimizer_reg = optim.Adam(regression_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"‚úì Regression setup complete\")\n",
    "print(f\"  Loss: MSE\")\n",
    "print(f\"  Optimizer: Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions for regression\n",
    "def train_epoch_regression(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mae = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_mae += torch.abs(outputs - targets).sum().item()\n",
    "        total += inputs.size(0)\n",
    "    \n",
    "    return running_loss / total, running_mae / total\n",
    "\n",
    "def validate_regression(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mae = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_mae += torch.abs(outputs - targets).sum().item()\n",
    "            total += inputs.size(0)\n",
    "    \n",
    "    return running_loss / total, running_mae / total\n",
    "\n",
    "print(\"‚úì Training functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression model\n",
    "num_epochs = 100\n",
    "train_losses_reg = []\n",
    "val_losses_reg = []\n",
    "train_maes_reg = []\n",
    "val_maes_reg = []\n",
    "\n",
    "print(\"Training regression model...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_mae = train_epoch_regression(\n",
    "        regression_model, train_loader_h, criterion_reg, optimizer_reg, device\n",
    "    )\n",
    "    val_loss, val_mae = validate_regression(\n",
    "        regression_model, test_loader_h, criterion_reg, device\n",
    "    )\n",
    "    \n",
    "    train_losses_reg.append(train_loss)\n",
    "    val_losses_reg.append(val_loss)\n",
    "    train_maes_reg.append(train_mae)\n",
    "    val_maes_reg.append(val_mae)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train MSE: {train_loss:.4f}, MAE: {train_mae:.4f}\")\n",
    "        print(f\"  Val MSE: {val_loss:.4f}, MAE: {val_mae:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses_reg, label='Training MSE', linewidth=2)\n",
    "ax1.plot(val_losses_reg, label='Validation MSE', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('MSE', fontsize=12)\n",
    "ax1.set_title('Regression Model MSE', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_maes_reg, label='Training MAE', linewidth=2)\n",
    "ax2.plot(val_maes_reg, label='Validation MAE', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('MAE', fontsize=12)\n",
    "ax2.set_title('Regression Model MAE', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "regression_model.eval()\n",
    "all_predictions_reg = []\n",
    "all_targets_reg = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader_h:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = regression_model(inputs)\n",
    "        all_predictions_reg.extend(outputs.cpu().numpy())\n",
    "        all_targets_reg.extend(targets.numpy())\n",
    "\n",
    "all_predictions_reg = np.array(all_predictions_reg).flatten()\n",
    "all_targets_reg = np.array(all_targets_reg).flatten()\n",
    "\n",
    "# Calculate R¬≤\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(all_targets_reg, all_predictions_reg)\n",
    "\n",
    "print(f\"Test MSE: {val_losses_reg[-1]:.4f}\")\n",
    "print(f\"Test MAE: {val_maes_reg[-1]:.4f} (${val_maes_reg[-1]*100000:.0f})\")\n",
    "print(f\"Test RMSE: {np.sqrt(val_losses_reg[-1]):.4f}\")\n",
    "print(f\"R¬≤ Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Predictions vs Actual\n",
    "ax1.scatter(all_targets_reg, all_predictions_reg, alpha=0.5, s=20)\n",
    "ax1.plot([all_targets_reg.min(), all_targets_reg.max()],\n",
    "         [all_targets_reg.min(), all_targets_reg.max()],\n",
    "         'r--', linewidth=2, label='Perfect predictions')\n",
    "ax1.set_xlabel('Actual Price ($100k)', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Price ($100k)', fontsize=12)\n",
    "ax1.set_title(f'Predictions vs Actual (R¬≤={r2:.3f})', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = all_targets_reg - all_predictions_reg\n",
    "ax2.scatter(all_predictions_reg, residuals, alpha=0.5, s=20)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Predicted Price ($100k)', fontsize=12)\n",
    "ax2.set_ylabel('Residuals', fontsize=12)\n",
    "ax2.set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Advanced Training Techniques\n",
    "\n",
    "Let's explore some advanced training techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedulers\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Example model and optimizer\n",
    "dummy_model = nn.Linear(10, 1)\n",
    "dummy_optimizer = optim.Adam(dummy_model.parameters(), lr=0.001)\n",
    "\n",
    "# StepLR: Reduce LR by factor every N epochs\n",
    "scheduler_step = StepLR(dummy_optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# ReduceLROnPlateau: Reduce when metric plateaus\n",
    "scheduler_plateau = ReduceLROnPlateau(dummy_optimizer, mode='min', \n",
    "                                     factor=0.5, patience=5)\n",
    "\n",
    "# CosineAnnealing: Cosine annealing schedule\n",
    "scheduler_cosine = CosineAnnealingLR(dummy_optimizer, T_max=50)\n",
    "\n",
    "print(\"Learning rate schedulers:\")\n",
    "print(f\"  StepLR: {scheduler_step}\")\n",
    "print(f\"  ReduceLROnPlateau: {scheduler_plateau}\")\n",
    "print(f\"  CosineAnnealingLR: {scheduler_cosine}\")\n",
    "\n",
    "print(\"\\nüí° Usage in training loop:\")\n",
    "print(\"  scheduler.step() after each epoch (StepLR, Cosine)\")\n",
    "print(\"  scheduler.step(val_loss) for ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient clipping prevents exploding gradients\n",
    "# Add this in your training loop:\n",
    "\n",
    "def train_with_clipping(model, dataloader, criterion, optimizer, device, max_norm=1.0):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "print(\"‚úì Gradient clipping example defined\")\n",
    "print(\"\\nüí° Use when gradients explode (loss becomes NaN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Training Loops with Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced training loop with more features\n",
    "from tqdm import tqdm\n",
    "\n",
    "def advanced_train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "print(\"‚úì Advanced training loop with progress bar defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Saving & Loading Models\n",
    "\n",
    "PyTorch provides flexible options for saving models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Save State Dict (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state dict\n",
    "torch.save(fashion_model.state_dict(), 'fashion_model.pth')\n",
    "print(\"‚úì Model state dict saved as 'fashion_model.pth'\")\n",
    "\n",
    "# Save with additional information\n",
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': fashion_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_fashion.state_dict(),\n",
    "    'loss': val_losses_fashion[-1],\n",
    "}\n",
    "torch.save(checkpoint, 'fashion_checkpoint.pth')\n",
    "print(\"‚úì Checkpoint saved as 'fashion_checkpoint.pth'\")\n",
    "\n",
    "print(\"\\nüí° .pth is the standard PyTorch model extension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model state dict\n",
    "loaded_model = FashionMNISTClassifier().to(device)\n",
    "loaded_model.load_state_dict(torch.load('fashion_model.pth', map_location=device))\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "\n",
    "# Verify it works\n",
    "val_loss, val_acc = validate_multiclass(\n",
    "    loaded_model, test_loader, criterion_fashion, device\n",
    ")\n",
    "print(f\"Loaded model accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_loaded = torch.load('fashion_checkpoint.pth', map_location=device)\n",
    "\n",
    "new_model = FashionMNISTClassifier().to(device)\n",
    "new_optimizer = optim.Adam(new_model.parameters())\n",
    "\n",
    "new_model.load_state_dict(checkpoint_loaded['model_state_dict'])\n",
    "new_optimizer.load_state_dict(checkpoint_loaded['optimizer_state_dict'])\n",
    "epoch = checkpoint_loaded['epoch']\n",
    "loss = checkpoint_loaded['loss']\n",
    "\n",
    "print(f\"Checkpoint loaded from epoch {epoch}\")\n",
    "print(f\"Previous loss: {loss:.4f}\")\n",
    "print(\"\\n‚úì You can continue training from this point!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Save Entire Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model (less flexible)\n",
    "torch.save(regression_model, 'regression_model_full.pth')\n",
    "print(\"‚úì Entire model saved\")\n",
    "\n",
    "# Load entire model\n",
    "loaded_full = torch.load('regression_model_full.pth', map_location=device, weights_only=False)\n",
    "loaded_full.eval()\n",
    "\n",
    "print(\"‚úì Full model loaded\")\n",
    "print(\"\\n‚ö†Ô∏è  Warning: This approach is less flexible\")\n",
    "print(\"   Prefer saving state_dict for production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Save State Dict (Recommended for Production)\n",
    "\n",
    "# Save model state_dict\n",
    "torch.save(regression_model.state_dict(), 'regression_model_state_dict.pth')\n",
    "print(\"‚úì Model state_dict saved\")\n",
    "\n",
    "# Load model state_dict\n",
    "# First, you need to recreate the model architecture\n",
    "loaded_model = RegressionModel(input_size=8)  # Instantiate your model class with the correct input_size\n",
    "loaded_model.load_state_dict(torch.load('regression_model_state_dict.pth',\n",
    "                                        map_location=device))\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"‚úì Model state_dict loaded\")\n",
    "print(\"‚úÖ This is the preferred approach for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Practices for Model Saving:\")\n",
    "print(\"\\n1. Save state_dict, not entire model\")\n",
    "print(\"   - More flexible\")\n",
    "print(\"   - Smaller file size\")\n",
    "print(\"   - Works across PyTorch versions\")\n",
    "print(\"\\n2. Include metadata in checkpoint\")\n",
    "print(\"   - Epoch number\")\n",
    "print(\"   - Optimizer state\")\n",
    "print(\"   - Best metrics\")\n",
    "print(\"\\n3. Use map_location when loading\")\n",
    "print(\"   - Prevents errors when GPU availability changes\")\n",
    "print(\"\\n4. Call model.eval() after loading\")\n",
    "print(\"   - Disables dropout and batch norm training mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13: Best Practices & Tips\n",
    "\n",
    "Let's wrap up with important best practices and debugging tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good practice: Define device at the start\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to device\n",
    "model = FashionMNISTClassifier().to(device)\n",
    "\n",
    "# Always move data to same device as model\n",
    "# inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "print(\"‚úì Device management tips:\")\n",
    "print(\"  1. Define device once at start\")\n",
    "print(\"  2. Move model with .to(device)\")\n",
    "print(\"  3. Move data in training loop\")\n",
    "print(\"  4. Use map_location when loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory Management Tips:\")\n",
    "print(\"\\n1. Use torch.no_grad() during evaluation\")\n",
    "print(\"   - Disables gradient computation\")\n",
    "print(\"   - Saves memory and speeds up inference\")\n",
    "print(\"\\n2. Delete unused variables\")\n",
    "print(\"   - del large_tensor\")\n",
    "print(\"   - torch.cuda.empty_cache() (GPU)\")\n",
    "print(\"\\n3. Use smaller batch sizes if OOM\")\n",
    "print(\"   - Reduce batch_size in DataLoader\")\n",
    "print(\"\\n4. Move tensors to CPU when done\")\n",
    "print(\"   - tensor.cpu() frees GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Errors and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Common PyTorch Errors:\\n\")\n",
    "\n",
    "print(\"1. RuntimeError: Expected tensor on cuda but got cpu\")\n",
    "print(\"   Solution: Ensure model and data are on same device\")\n",
    "print(\"   Fix: inputs = inputs.to(device)\\n\")\n",
    "\n",
    "print(\"2. RuntimeError: size mismatch\")\n",
    "print(\"   Solution: Check input/output dimensions\")\n",
    "print(\"   Debug: print(tensor.shape) at each layer\\n\")\n",
    "\n",
    "print(\"3. Loss is NaN\")\n",
    "print(\"   Solutions:\")\n",
    "print(\"   - Lower learning rate\")\n",
    "print(\"   - Use gradient clipping\")\n",
    "print(\"   - Check for inf/nan in data\\n\")\n",
    "\n",
    "print(\"4. CUDA out of memory\")\n",
    "print(\"   Solutions:\")\n",
    "print(\"   - Reduce batch_size\")\n",
    "print(\"   - Use torch.cuda.empty_cache()\")\n",
    "print(\"   - Use gradient accumulation\\n\")\n",
    "\n",
    "print(\"5. Model not learning (loss not decreasing)\")\n",
    "print(\"   Solutions:\")\n",
    "print(\"   - Check learning rate (try higher)\")\n",
    "print(\"   - Verify data normalization\")\n",
    "print(\"   - Check loss function matches task\")\n",
    "print(\"   - Ensure optimizer.zero_grad() is called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch vs Keras Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch vs Keras:\\n\")\n",
    "\n",
    "print(\"PyTorch Advantages:\")\n",
    "print(\"  ‚úì More explicit and transparent\")\n",
    "print(\"  ‚úì Better for research and custom architectures\")\n",
    "print(\"  ‚úì Dynamic computation graphs\")\n",
    "print(\"  ‚úì More Pythonic\")\n",
    "print(\"  ‚úì Better debugging\")\n",
    "\n",
    "print(\"\\nKeras Advantages:\")\n",
    "print(\"  ‚úì Simpler, more beginner-friendly\")\n",
    "print(\"  ‚úì Less boilerplate code\")\n",
    "print(\"  ‚úì Faster prototyping\")\n",
    "print(\"  ‚úì Built-in training loop\")\n",
    "print(\"  ‚úì Excellent for standard architectures\")\n",
    "\n",
    "print(\"\\nWhen to use which:\")\n",
    "print(\"  - PyTorch: Research, custom models, need control\")\n",
    "print(\"  - Keras: Quick prototyping, standard models, beginners\")\n",
    "print(\"\\nüí° Both are excellent - choose based on your needs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Quick Reference:\\n\")\n",
    "\n",
    "print(\"Model Definition:\")\n",
    "print(\"  class Model(nn.Module):\")\n",
    "print(\"      def __init__(self): ...\")\n",
    "print(\"      def forward(self, x): ...\\n\")\n",
    "\n",
    "print(\"Training Loop:\")\n",
    "print(\"  model.train()\")\n",
    "print(\"  optimizer.zero_grad()\")\n",
    "print(\"  outputs = model(inputs)\")\n",
    "print(\"  loss = criterion(outputs, labels)\")\n",
    "print(\"  loss.backward()\")\n",
    "print(\"  optimizer.step()\\n\")\n",
    "\n",
    "print(\"Evaluation:\")\n",
    "print(\"  model.eval()\")\n",
    "print(\"  with torch.no_grad():\")\n",
    "print(\"      outputs = model(inputs)\\n\")\n",
    "\n",
    "print(\"Save/Load:\")\n",
    "print(\"  torch.save(model.state_dict(), 'model.pth')\")\n",
    "print(\"  model.load_state_dict(torch.load('model.pth'))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations! üî•\n",
    "\n",
    "You've completed the PyTorch ANN tutorial! You now know how to:\n",
    "\n",
    "‚úÖ Work with PyTorch tensors and autograd  \n",
    "‚úÖ Build neural networks using nn.Module  \n",
    "‚úÖ Create custom training and evaluation loops  \n",
    "‚úÖ Use DataLoader for efficient data handling  \n",
    "‚úÖ Train models for classification and regression  \n",
    "‚úÖ Apply Dropout and Batch Normalization  \n",
    "‚úÖ Optimize with different optimizers and schedulers  \n",
    "‚úÖ Save and load models  \n",
    "‚úÖ Debug common errors  \n",
    "‚úÖ Understand PyTorch vs Keras tradeoffs  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Compare this notebook with the Keras tutorial\n",
    "2. Experiment with different architectures and hyperparameters\n",
    "3. Try custom datasets\n",
    "4. Learn about Convolutional Neural Networks (CNNs)\n",
    "5. Explore Recurrent Neural Networks (RNNs)\n",
    "6. Dive into Transfer Learning\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "- [Deep Learning with PyTorch Book](https://pytorch.org/deep-learning-with-pytorch)\n",
    "\n",
    "Keep building and experimenting! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
