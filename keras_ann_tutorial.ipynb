{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial: Artificial Neural Networks (ANNs)\n",
    "\n",
    "## Welcome to Keras!\n",
    "\n",
    "This comprehensive tutorial will teach you how to build, train, and evaluate Artificial Neural Networks using **Keras 3** with TensorFlow backend.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Build neural network models using Sequential and Functional APIs\n",
    "- Understand and use different layers, activations, and optimizers\n",
    "- Train models for classification and regression tasks\n",
    "- Evaluate model performance and make predictions\n",
    "- Apply regularization techniques\n",
    "- Save and load trained models\n",
    "\n",
    "### Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of basic machine learning concepts\n",
    "- NumPy fundamentals (helpful)\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Introduction & Setup\n",
    "\n",
    "First, let's import the necessary libraries and check that everything is installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Core libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Keras version: 3.13.0\n",
      "\n",
      "Keras backend: tensorflow\n",
      "\n",
      "âœ“ Keras is ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers, losses, metrics\n",
    "\n",
    "# Check versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"\\nKeras backend: {keras.backend.backend()}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"\\nâœ“ Keras is ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Keras 3\n",
    "\n",
    "**Keras 3** is a multi-backend deep learning framework that can run on:\n",
    "- **TensorFlow** (default, what we're using)\n",
    "- **JAX** (for high-performance computing)\n",
    "- **PyTorch** (for PyTorch enthusiasts)\n",
    "\n",
    "This makes Keras extremely flexible! For this tutorial, we're using the TensorFlow backend, which is the most popular and well-supported option.\n",
    "\n",
    "**Key advantages of Keras:**\n",
    "- ðŸŽ¯ User-friendly API - easy to learn\n",
    "- ðŸ—ï¸ Modular design - build complex models easily  \n",
    "- ðŸ“š Excellent documentation\n",
    "- ðŸš€ Production-ready with TensorFlow ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Data Fundamentals\n",
    "\n",
    "Before building models, we need to understand how to work with data in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset - MNIST (handwritten digits)\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load data - returns train and test splits\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Image shape: {X_train.shape[1:]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Sample MNIST Digits', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i], cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each image is a 28x28 pixel grayscale image of a handwritten digit (0-9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Neural networks work best when:\n",
    "1. **Data is normalized** (scaled to 0-1 or -1 to 1)\n",
    "2. **Data is properly shaped** (flattened for ANNs)\n",
    "3. **Labels are in the correct format** (one-hot encoded for multi-class)\n",
    "\n",
    "Let's prepare our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Flatten images from 28x28 to 784 (for ANN input)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)  # -1 means \"infer dimension\"\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Original shape: {X_train.shape}\")\n",
    "print(f\"Flattened shape: {X_train_flat.shape}\")\n",
    "print(f\"\\nEach image is now a vector of {X_train_flat.shape[1]} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize pixel values from [0, 255] to [0, 1]\n",
    "X_train_norm = X_train_flat.astype('float32') / 255.0\n",
    "X_test_norm = X_test_flat.astype('float32') / 255.0\n",
    "\n",
    "print(f\"Original pixel range: [{X_train_flat.min()}, {X_train_flat.max()}]\")\n",
    "print(f\"Normalized pixel range: [{X_train_norm.min():.2f}, {X_train_norm.max():.2f}]\")\n",
    "print(\"\\nâœ“ Data normalized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert labels to categorical (one-hot encoding)\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Before: labels are integers (0-9)\n",
    "print(f\"Original labels: {y_train[:5]}\")\n",
    "\n",
    "# After: labels are one-hot vectors\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"\\nOne-hot encoded label for '{y_train[0]}':\")\n",
    "print(y_train_cat[0])\n",
    "print(f\"\\nLabel shape: {y_train_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why one-hot encoding?**  \n",
    "For multi-class classification, we convert integer labels (like 5) to vectors like `[0,0,0,0,0,1,0,0,0,0]`. This helps the network learn that classes are distinct categories, not ordered numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Building Your First Model - Sequential API\n",
    "\n",
    "Now let's build our first neural network! We'll use the **Sequential API**, which is perfect for models where data flows linearly through layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Neural Network Architecture\n",
    "\n",
    "An Artificial Neural Network (ANN) consists of:\n",
    "- **Input Layer**: Receives the data (784 neurons for our flattened 28x28 images)\n",
    "- **Hidden Layers**: Process the data (we choose how many neurons)\n",
    "- **Output Layer**: Produces predictions (10 neurons for 10 digit classes)\n",
    "\n",
    "Let's build a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),  # Hidden layer 1\n",
    "    layers.Dense(64, activation='relu'),                        # Hidden layer 2  \n",
    "    layers.Dense(10, activation='softmax')                      # Output layer\n",
    "])\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(\"\\nLet's examine the architecture:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model architecture\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key observations:\")\n",
    "print(\"- Total parameters: ~100K (these are learned during training)\")\n",
    "print(\"- Most parameters are in the first layer (784 inputs Ã— 128 neurons)\")\n",
    "print(\"- Output layer has 10 neurons (one for each digit class)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Layers\n",
    "\n",
    "Let's break down what each layer does:\n",
    "\n",
    "1. **`Dense(128, activation='relu', input_shape=(784,))`**\n",
    "   - `Dense`: Fully connected layer (every input connects to every output)\n",
    "   - `128`: Number of neurons in this layer\n",
    "   - `activation='relu'`: ReLU activation function (more on this later)\n",
    "   - `input_shape=(784,)`: Expected input dimension (only needed for first layer)\n",
    "\n",
    "2. **`Dense(64, activation='relu')`**\n",
    "   - Second hidden layer with 64 neurons\n",
    "   - No input_shape needed (Keras infers it from previous layer)\n",
    "\n",
    "3. **`Dense(10, activation='softmax')`**\n",
    "   - Output layer with 10 neurons (one per class)\n",
    "   - `softmax`: Converts outputs to probabilities that sum to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 1: Binary Classification with Iris Dataset\n",
    "\n",
    "Let's apply what we learned to a simpler problem: binary classification (2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification: Setosa (0) vs. Not Setosa (1 or 2)\n",
    "y_binary = (y != 0).astype(int)\n",
    "\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Setosa: {np.sum(y_binary == 0)}\")\n",
    "print(f\"  Not Setosa: {np.sum(y_binary == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features (important for neural networks!)\n",
    "scaler = StandardScaler()\n",
    "X_train_iris = scaler.fit_transform(X_train_iris)\n",
    "X_test_iris = scaler.transform(X_test_iris)\n",
    "\n",
    "print(f\"Training samples: {len(X_train_iris)}\")\n",
    "print(f\"Test samples: {len(X_test_iris)}\")\n",
    "print(\"\\nâœ“ Data prepared for binary classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build binary classification model\n",
    "binary_model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(4,)),  # 4 input features\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # sigmoid for binary classification\n",
    "], name='binary_classifier')\n",
    "\n",
    "binary_model.summary()\n",
    "\n",
    "print(\"\\nðŸ’¡ Note: For binary classification, we use:\")\n",
    "print(\"  - 1 output neuron (instead of 2)\")\n",
    "print(\"  - sigmoid activation (outputs probability between 0 and 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Model Compilation\n",
    "\n",
    "Before training, we must **compile** the model by specifying:\n",
    "1. **Optimizer**: How the model updates its weights\n",
    "2. **Loss function**: What the model tries to minimize\n",
    "3. **Metrics**: What we use to evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Optimizers\n",
    "\n",
    "**Common optimizers:**\n",
    "- **SGD**: Stochastic Gradient Descent (classic, requires careful tuning)\n",
    "- **Adam**: Adaptive Moment Estimation (most popular, works well out-of-the-box)\n",
    "- **RMSprop**: Root Mean Square Propagation (good for RNNs)\n",
    "\n",
    "**For beginners, use Adam** - it's fast and reliable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the binary classification model\n",
    "binary_model.compile(\n",
    "    optimizer='adam',                    # Adam optimizer with default learning rate (0.001)\n",
    "    loss='binary_crossentropy',          # Loss for binary classification\n",
    "    metrics=['accuracy']                 # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model compiled successfully!\")\n",
    "print(\"\\nCompilation settings:\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss: Binary Crossentropy\")\n",
    "print(f\"  Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions Guide\n",
    "\n",
    "Choose your loss function based on the problem:\n",
    "\n",
    "| Problem Type | Output Activation | Loss Function |\n",
    "|--------------|-------------------|---------------|\n",
    "| Binary Classification | `sigmoid` | `binary_crossentropy` |\n",
    "| Multi-class Classification | `softmax` | `categorical_crossentropy` |\n",
    "| Multi-class (integer labels) | `softmax` | `sparse_categorical_crossentropy` |\n",
    "| Regression | None or `linear` | `mean_squared_error` or `mae` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compile with different optimizers and learning rates\n",
    "\n",
    "# Option 1: Adam with custom learning rate\n",
    "# binary_model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# Option 2: SGD with momentum\n",
    "# binary_model.compile(\n",
    "#     optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "print(\"ðŸ’¡ You can experiment with different optimizers and hyperparameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Training Models\n",
    "\n",
    "Now comes the exciting part - training our model! We use the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the binary classification model\n",
    "history = binary_model.fit(\n",
    "    X_train_iris,           # Training data\n",
    "    y_train_iris,           # Training labels\n",
    "    epochs=50,              # Number of times to iterate over the entire dataset\n",
    "    batch_size=16,          # Number of samples per gradient update\n",
    "    validation_split=0.2,   # Use 20% of training data for validation\n",
    "    verbose=1               # Print progress (0=silent, 1=progress bar, 2=one line per epoch)\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Training Parameters\n",
    "\n",
    "- **`epochs`**: How many times the model sees the entire dataset\n",
    "  - More epochs = more learning (but risk of overfitting)\n",
    "  - Start with 10-50 epochs\n",
    "\n",
    "- **`batch_size`**: How many samples to process before updating weights\n",
    "  - Smaller batches (16-32): More updates, better generalization\n",
    "  - Larger batches (128-256): Faster training, more stable\n",
    "\n",
    "- **`validation_split`**: Percentage of training data to use for validation\n",
    "  - Helps monitor overfitting\n",
    "  - Typical values: 0.1 to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Good signs:\")\n",
    "print(\"  - Loss decreases over time\")\n",
    "print(\"  - Training and validation curves are close (no overfitting)\")\n",
    "print(\"  - Accuracy increases over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Using validation_data instead of validation_split\n",
    "# This gives you more control over the validation set\n",
    "\n",
    "# history = binary_model.fit(\n",
    "#     X_train_iris,\n",
    "#     y_train_iris,\n",
    "#     epochs=50,\n",
    "#     batch_size=16,\n",
    "#     validation_data=(X_test_iris, y_test_iris)  # Use test set for validation\n",
    "# )\n",
    "\n",
    "print(\"ðŸ’¡ You can also pass validation_data=(X_val, y_val) explicitly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Model Evaluation & Prediction\n",
    "\n",
    "After training, we need to evaluate how well our model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = binary_model.evaluate(X_test_iris, y_test_iris, verbose=0)\n",
    "\n",
    "print(\"ðŸ“Š Test Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nâœ“ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = binary_model.predict(X_test_iris, verbose=0)\n",
    "\n",
    "print(\"Sample predictions (probabilities):\")\n",
    "print(predictions[:5].flatten())\n",
    "\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nPredicted classes:\")\n",
    "print(predicted_classes[:10])\n",
    "\n",
    "print(\"\\nActual classes:\")\n",
    "print(y_test_iris[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_iris, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_iris, predicted_classes, \n",
    "                          target_names=['Setosa', 'Not Setosa']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Predictions\n",
    "\n",
    "For **binary classification**:\n",
    "- `model.predict()` returns probabilities (values between 0 and 1)\n",
    "- We convert to class labels using a threshold (typically 0.5)\n",
    "- Probability > 0.5 â†’ Class 1, Probability â‰¤ 0.5 â†’ Class 0\n",
    "\n",
    "For **multi-class classification** (coming up next):\n",
    "- `model.predict()` returns probability for each class\n",
    "- We use `argmax()` to get the class with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Making predictions on new data\n",
    "new_sample = X_test_iris[0:1]  # Take first test sample\n",
    "\n",
    "prediction_prob = binary_model.predict(new_sample, verbose=0)[0][0]\n",
    "prediction_class = int(prediction_prob > 0.5)\n",
    "\n",
    "print(f\"New sample features: {new_sample[0]}\")\n",
    "print(f\"\\nPredicted probability: {prediction_prob:.4f}\")\n",
    "print(f\"Predicted class: {prediction_class} ({'Not Setosa' if prediction_class else 'Setosa'})\")\n",
    "print(f\"Actual class: {y_test_iris[0]} ({'Not Setosa' if y_test_iris[0] else 'Setosa'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Functional API\n",
    "\n",
    "The **Functional API** is more flexible than Sequential API. It's useful for:\n",
    "- Models with multiple inputs/outputs\n",
    "- Models with shared layers\n",
    "- Non-linear topologies (e.g., residual connections)\n",
    "\n",
    "Let's rebuild our binary classifier using Functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API approach\n",
    "from keras import Input, Model\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=(4,), name='input_layer')\n",
    "\n",
    "# Build architecture by calling layers on previous layer\n",
    "x = layers.Dense(16, activation='relu', name='hidden_1')(inputs)\n",
    "x = layers.Dense(8, activation='relu', name='hidden_2')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "# Create model\n",
    "functional_model = Model(inputs=inputs, outputs=outputs, name='functional_binary_classifier')\n",
    "\n",
    "functional_model.summary()\n",
    "\n",
    "print(\"\\nâœ“ Functional model created!\")\n",
    "print(\"\\nðŸ’¡ Notice: Same architecture, different syntax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential vs Functional API\n",
    "\n",
    "**Sequential API:**\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(4,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "**Functional API:**\n",
    "```python\n",
    "inputs = Input(shape=(4,))\n",
    "x = layers.Dense(16, activation='relu')(inputs)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "**When to use which?**\n",
    "- Use **Sequential** for simple, linear models (most cases)\n",
    "- Use **Functional** for complex architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the functional model\n",
    "functional_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train (quietly this time)\n",
    "history_func = functional_model.fit(\n",
    "    X_train_iris, y_train_iris,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = functional_model.evaluate(X_test_iris, y_test_iris, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nâœ“ Functional model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Multi-Input Model (Advanced Preview)\n",
    "\n",
    "Here's a preview of what Functional API enables (you don't need to run this now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of multi-input model (for illustration only)\n",
    "\n",
    "# Two separate inputs\n",
    "input_1 = Input(shape=(10,), name='input_1')\n",
    "input_2 = Input(shape=(5,), name='input_2')\n",
    "\n",
    "# Process each input separately\n",
    "x1 = layers.Dense(32, activation='relu')(input_1)\n",
    "x2 = layers.Dense(16, activation='relu')(input_2)\n",
    "\n",
    "# Merge the two paths\n",
    "merged = layers.concatenate([x1, x2])\n",
    "\n",
    "# Final output\n",
    "output = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# Create multi-input model\n",
    "multi_input_model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "print(\"Multi-input model architecture:\")\n",
    "multi_input_model.summary()\n",
    "\n",
    "print(\"\\nðŸ’¡ This is just a preview - Sequential API can't do this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Common Layers & Techniques\n",
    "\n",
    "Let's explore important layers and techniques for building robust neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce **non-linearity** to the network, allowing it to learn complex patterns.\n",
    "\n",
    "**Common activations:**\n",
    "- **ReLU** (Rectified Linear Unit): `max(0, x)` - most popular for hidden layers\n",
    "- **Sigmoid**: Outputs between 0 and 1 - used for binary classification\n",
    "- **Tanh**: Outputs between -1 and 1 - alternative to ReLU\n",
    "- **Softmax**: Converts to probabilities - used for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# ReLU\n",
    "axes[0, 0].plot(x, np.maximum(0, x), linewidth=2)\n",
    "axes[0, 0].set_title('ReLU: max(0, x)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 1].plot(x, 1 / (1 + np.exp(-x)), linewidth=2)\n",
    "axes[0, 1].set_title('Sigmoid: 1 / (1 + e^(-x))', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=0.5, color='r', linewidth=0.5, linestyle='--')\n",
    "\n",
    "# Tanh\n",
    "axes[1, 0].plot(x, np.tanh(x), linewidth=2)\n",
    "axes[1, 0].set_title('Tanh: (e^x - e^(-x)) / (e^x + e^(-x))', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Linear\n",
    "axes[1, 1].plot(x, x, linewidth=2)\n",
    "axes[1, 1].set_title('Linear: f(x) = x', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Choose activation based on layer:\")\n",
    "print(\"  - Hidden layers: ReLU (default choice)\")\n",
    "print(\"  - Binary output: Sigmoid\")\n",
    "print(\"  - Multi-class output: Softmax\")\n",
    "print(\"  - Regression output: Linear (or none)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout - Regularization Technique\n",
    "\n",
    "**Dropout** randomly \"drops\" (sets to 0) a percentage of neurons during training. This:\n",
    "- Prevents overfitting\n",
    "- Makes the network more robust\n",
    "- Acts like training an ensemble of networks\n",
    "\n",
    "Typical dropout rate: 0.2 to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with Dropout\n",
    "model_with_dropout = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.3),  # Drop 30% of neurons randomly\n",
    "    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='model_with_dropout')\n",
    "\n",
    "model_with_dropout.summary()\n",
    "\n",
    "print(\"\\nðŸ’¡ Dropout layers have no trainable parameters\")\n",
    "print(\"   They only activate during training, not during inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "**Batch Normalization** normalizes the inputs of each layer. Benefits:\n",
    "- Faster training\n",
    "- Higher learning rates possible\n",
    "- Less sensitive to initialization\n",
    "- Acts as regularization\n",
    "\n",
    "Place it after Dense layers (before or after activation - both work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with Batch Normalization\n",
    "model_with_bn = keras.Sequential([\n",
    "    layers.Dense(128, input_shape=(784,)),\n",
    "    layers.BatchNormalization(),  # Normalize outputs\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='model_with_batchnorm')\n",
    "\n",
    "model_with_bn.summary()\n",
    "\n",
    "print(\"\\nðŸ’¡ BatchNormalization has trainable parameters (gamma and beta)\")\n",
    "print(\"   It learns the optimal normalization for each layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Techniques\n",
    "\n",
    "You can combine multiple techniques for better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced model with multiple techniques\n",
    "advanced_model = keras.Sequential([\n",
    "    layers.Dense(128, input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(64),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='advanced_model')\n",
    "\n",
    "advanced_model.summary()\n",
    "\n",
    "print(\"\\nâœ“ This model combines:\")\n",
    "print(\"  - Batch Normalization (faster training, regularization)\")\n",
    "print(\"  - Dropout (prevent overfitting)\")\n",
    "print(\"  - ReLU activation (non-linearity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Complete Project - Multi-Class Classification\n",
    "\n",
    "Let's put everything together in a complete MNIST digit classification project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already loaded and preprocessed MNIST earlier\n",
    "print(\"Data shapes:\")\n",
    "print(f\"  X_train: {X_train_norm.shape}\")\n",
    "print(f\"  y_train: {y_train_cat.shape}\")\n",
    "print(f\"  X_test: {X_test_norm.shape}\")\n",
    "print(f\"  y_test: {y_test_cat.shape}\")\n",
    "print(\"\\nâœ“ Data ready for multi-class classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final MNIST classifier\n",
    "mnist_model = keras.Sequential([\n",
    "    # Input layer + first hidden layer\n",
    "    layers.Dense(256, input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    layers.Dense(128),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Third hidden layer\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Output layer\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='mnist_classifier')\n",
    "\n",
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "mnist_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model compiled and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history_mnist = mnist_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_cat,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history_mnist.history['loss'], label='Training', linewidth=2)\n",
    "ax1.plot(history_mnist.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('MNIST Model Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history_mnist.history['accuracy'], label='Training', linewidth=2)\n",
    "ax2.plot(history_mnist.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('MNIST Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = mnist_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"ðŸ“Š Final Test Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_mnist = mnist_model.predict(X_test_norm, verbose=0)\n",
    "\n",
    "# Convert from one-hot to class labels\n",
    "predicted_classes = np.argmax(predictions_mnist, axis=1)\n",
    "true_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "print(f\"Predicted: {predicted_classes[:10]}\")\n",
    "print(f\"Actual:    {true_classes[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.suptitle('MNIST Predictions', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test[i], cmap='gray')\n",
    "    \n",
    "    # Color based on correctness\n",
    "    color = 'green' if predicted_classes[i] == true_classes[i] else 'red'\n",
    "    confidence = predictions_mnist[i][predicted_classes[i]] * 100\n",
    "    \n",
    "    ax.set_title(f'Pred: {predicted_classes[i]} ({confidence:.1f}%)\\nTrue: {true_classes[i]}',\n",
    "                color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green = Correct, Red = Incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('MNIST Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Diagonal values are correct predictions\")\n",
    "print(\"   Off-diagonal values are errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_idx = np.where(predicted_classes != true_classes)[0]\n",
    "\n",
    "print(f\"Total misclassified: {len(misclassified_idx)} / {len(true_classes)}\")\n",
    "print(f\"Error rate: {len(misclassified_idx) / len(true_classes) * 100:.2f}%\")\n",
    "\n",
    "# Show some errors\n",
    "if len(misclassified_idx) > 0:\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle('Misclassified Examples', fontsize=16, fontweight='bold', color='red')\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(misclassified_idx):\n",
    "            idx = misclassified_idx[i]\n",
    "            ax.imshow(X_test[idx], cmap='gray')\n",
    "            confidence = predictions_mnist[idx][predicted_classes[idx]] * 100\n",
    "            ax.set_title(f'Predicted: {predicted_classes[idx]} ({confidence:.1f}%)\\nActual: {true_classes[idx]}',\n",
    "                        color='red', fontweight='bold')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Regression with ANNs\n",
    "\n",
    "Now let's tackle a different type of problem: **regression** (predicting continuous values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "\n",
    "print(\"California Housing Dataset\")\n",
    "print(f\"Samples: {X_housing.shape[0]}\")\n",
    "print(f\"Features: {X_housing.shape[1]}\")\n",
    "print(f\"\\nFeature names: {housing.feature_names}\")\n",
    "print(f\"\\nTarget: Median house value (in $100,000s)\")\n",
    "print(f\"Target range: ${y_housing.min()*100000:.0f} - ${y_housing.max()*100000:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_housing, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('House Value ($100k)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of House Prices', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(y_housing, vert=True)\n",
    "plt.ylabel('House Value ($100k)', fontsize=12)\n",
    "plt.title('House Price Box Plot', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and normalize data\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features (CRITICAL for regression!)\n",
    "scaler_housing = StandardScaler()\n",
    "X_train_h = scaler_housing.fit_transform(X_train_h)\n",
    "X_test_h = scaler_housing.transform(X_test_h)\n",
    "\n",
    "print(f\"Training samples: {len(X_train_h)}\")\n",
    "print(f\"Test samples: {len(X_test_h)}\")\n",
    "print(\"\\nâœ“ Data prepared for regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences for Regression\n",
    "\n",
    "Compared to classification:\n",
    "\n",
    "1. **Output layer**: 1 neuron (instead of multiple classes)\n",
    "2. **Output activation**: None or linear (instead of softmax/sigmoid)\n",
    "3. **Loss function**: MSE or MAE (instead of crossentropy)\n",
    "4. **Metrics**: MAE, MSE, RÂ² (instead of accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build regression model\n",
    "regression_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(8,)),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(16, activation='relu'),\n",
    "    \n",
    "    # Output layer for regression\n",
    "    layers.Dense(1)  # No activation (linear output)\n",
    "], name='housing_price_predictor')\n",
    "\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with regression-specific settings\n",
    "regression_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',  # Mean Squared Error\n",
    "    metrics=['mae']  # Mean Absolute Error\n",
    ")\n",
    "\n",
    "print(\"âœ“ Regression model compiled!\")\n",
    "print(\"\\nRegression settings:\")\n",
    "print(\"  Loss: MSE (penalizes large errors more)\")\n",
    "print(\"  Metric: MAE (average absolute error in $100k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the regression model\n",
    "history_reg = regression_model.fit(\n",
    "    X_train_h,\n",
    "    y_train_h,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MSE\n",
    "ax1.plot(history_reg.history['loss'], label='Training MSE', linewidth=2)\n",
    "ax1.plot(history_reg.history['val_loss'], label='Validation MSE', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('MSE', fontsize=12)\n",
    "ax1.set_title('Mean Squared Error', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "ax2.plot(history_reg.history['mae'], label='Training MAE', linewidth=2)\n",
    "ax2.plot(history_reg.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('MAE', fontsize=12)\n",
    "ax2.set_title('Mean Absolute Error', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_mse, test_mae = regression_model.evaluate(X_test_h, y_test_h, verbose=0)\n",
    "\n",
    "print(\"ðŸ“Š Test Results:\")\n",
    "print(f\"  MSE: {test_mse:.4f}\")\n",
    "print(f\"  MAE: {test_mae:.4f} (average error of ${test_mae*100000:.0f})\")\n",
    "print(f\"  RMSE: {np.sqrt(test_mse):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_reg = regression_model.predict(X_test_h, verbose=0).flatten()\n",
    "\n",
    "# Calculate RÂ² score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test_h, predictions_reg)\n",
    "\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "print(f\"\\nðŸ’¡ RÂ² = {r2:.2%} of variance explained by the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax1.scatter(y_test_h, predictions_reg, alpha=0.5, s=20)\n",
    "ax1.plot([y_test_h.min(), y_test_h.max()], \n",
    "         [y_test_h.min(), y_test_h.max()], \n",
    "         'r--', linewidth=2, label='Perfect predictions')\n",
    "ax1.set_xlabel('Actual Price ($100k)', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Price ($100k)', fontsize=12)\n",
    "ax1.set_title(f'Predictions vs Actual (RÂ²={r2:.3f})', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test_h - predictions_reg\n",
    "ax2.scatter(predictions_reg, residuals, alpha=0.5, s=20)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Predicted Price ($100k)', fontsize=12)\n",
    "ax2.set_ylabel('Residuals', fontsize=12)\n",
    "ax2.set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Good model: Points close to red line, residuals centered around 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"\\nActual | Predicted | Error\")\n",
    "print(\"-\" * 35)\n",
    "for i in range(10):\n",
    "    actual = y_test_h[i] * 100000\n",
    "    pred = predictions_reg[i] * 100000\n",
    "    error = abs(actual - pred)\n",
    "    print(f\"${actual:,.0f} | ${pred:,.0f} | ${error:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Saving & Loading Models\n",
    "\n",
    "After training a model, you'll want to save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in Keras format (recommended)\n",
    "mnist_model.save('mnist_model.keras')\n",
    "print(\"âœ“ Model saved as 'mnist_model.keras'\")\n",
    "\n",
    "# Save regression model\n",
    "regression_model.save('housing_model.keras')\n",
    "print(\"âœ“ Regression model saved as 'housing_model.keras'\")\n",
    "\n",
    "print(\"\\nðŸ’¡ .keras format includes:\")\n",
    "print(\"  - Model architecture\")\n",
    "print(\"  - Model weights\")\n",
    "print(\"  - Optimizer state\")\n",
    "print(\"  - Compilation settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_model = keras.models.load_model('mnist_model.keras')\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")\n",
    "print(\"\\nModel summary:\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify loaded model works\n",
    "test_loss, test_acc = loaded_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"Loaded model accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nâœ“ Loaded model performs identically to original!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Save Only Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only weights (smaller file size)\n",
    "mnist_model.save_weights('mnist_weights.weights.h5')\n",
    "print(\"âœ“ Weights saved as 'mnist_weights.weights.h5'\")\n",
    "\n",
    "print(\"\\nðŸ’¡ To load weights:\")\n",
    "print(\"  1. Recreate the model architecture\")\n",
    "print(\"  2. Load weights: model.load_weights('mnist_weights.weights.h5')\")\n",
    "print(\"  3. Compile the model (if needed for training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load weights into a new model\n",
    "new_model = keras.Sequential([\n",
    "    layers.Dense(256, input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Load weights\n",
    "new_model.load_weights('mnist_weights.weights.h5')\n",
    "\n",
    "# Compile (required for training or evaluation)\n",
    "new_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = new_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "print(f\"\\nModel with loaded weights accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Best Practices & Tips\n",
    "\n",
    "Let's cover important concepts for building better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting vs Underfitting\n",
    "\n",
    "**Underfitting** (high bias):\n",
    "- Model is too simple\n",
    "- Poor performance on both training and test data\n",
    "- Solution: Make model more complex, train longer\n",
    "\n",
    "**Overfitting** (high variance):\n",
    "- Model is too complex\n",
    "- Great on training data, poor on test data\n",
    "- Solution: Regularization (Dropout, L2), more data, early stopping\n",
    "\n",
    "**Signs of overfitting:**\n",
    "- Training accuracy >> Validation accuracy\n",
    "- Validation loss increases while training loss decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Callbacks\n",
    "\n",
    "Callbacks are functions that run during training. Common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Early Stopping: Stop training when validation loss stops improving\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,  # Wait 5 epochs with no improvement\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model Checkpoint: Save best model during training\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce Learning Rate: Lower LR when learning plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Multiply LR by 0.5\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"âœ“ Callbacks defined\")\n",
    "print(\"\\nTo use them:\")\n",
    "print(\"  model.fit(..., callbacks=[early_stop, checkpoint, reduce_lr])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train with callbacks\n",
    "example_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(8,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "example_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train with callbacks\n",
    "history_cb = example_model.fit(\n",
    "    X_train_h, y_train_h,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining stopped at epoch {len(history_cb.history['loss'])}\")\n",
    "print(\"Best model saved automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Tuning\n",
    "\n",
    "Learning rate is one of the most important hyperparameters:\n",
    "\n",
    "- **Too high**: Training is unstable, loss oscillates\n",
    "- **Too low**: Training is very slow\n",
    "- **Just right**: Smooth, steady improvement\n",
    "\n",
    "**Good starting values:**\n",
    "- Adam: 0.001 (default)\n",
    "- SGD: 0.01 to 0.1\n",
    "\n",
    "**Tips:**\n",
    "- Start with default\n",
    "- If training is slow, try higher LR\n",
    "- If loss is unstable, try lower LR\n",
    "- Use ReduceLROnPlateau callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Errors and Solutions\n",
    "\n",
    "**1. Shape Mismatch**\n",
    "```\n",
    "Error: expected input_shape=(784,), got (28, 28)\n",
    "Solution: Flatten your data or adjust input_shape\n",
    "```\n",
    "\n",
    "**2. NaN Loss**\n",
    "```\n",
    "Cause: Learning rate too high, data not normalized\n",
    "Solution: Lower LR, normalize data, check for inf/nan in data\n",
    "```\n",
    "\n",
    "**3. Not Learning (loss not decreasing)**\n",
    "```\n",
    "Causes: LR too low, data not normalized, wrong loss function\n",
    "Solutions: Increase LR, normalize data, verify loss function\n",
    "```\n",
    "\n",
    "**4. Overfitting**\n",
    "```\n",
    "Solutions: Add Dropout, get more data, reduce model complexity,\n",
    "           add L2 regularization, early stopping\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reference\n",
    "\n",
    "**Model Building:**\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units, activation='relu', input_shape=(n_features,)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(n_classes, activation='softmax')  # or 'sigmoid' or None\n",
    "])\n",
    "```\n",
    "\n",
    "**Compilation:**\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',  # or SGD, RMSprop\n",
    "    loss='categorical_crossentropy',  # or 'mse', 'binary_crossentropy'\n",
    "    metrics=['accuracy']  # or 'mae'\n",
    ")\n",
    "```\n",
    "\n",
    "**Training:**\n",
    "```python\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[...]\n",
    ")\n",
    "```\n",
    "\n",
    "**Evaluation:**\n",
    "```python\n",
    "loss, metric = model.evaluate(X_test, y_test)\n",
    "predictions = model.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've completed the Keras ANN tutorial! You now know how to:\n",
    "\n",
    "âœ… Build neural networks using Sequential and Functional APIs  \n",
    "âœ… Preprocess data for neural networks  \n",
    "âœ… Choose appropriate activations, losses, and optimizers  \n",
    "âœ… Train models with validation  \n",
    "âœ… Evaluate model performance  \n",
    "âœ… Apply regularization techniques  \n",
    "âœ… Handle classification and regression tasks  \n",
    "âœ… Save and load models  \n",
    "âœ… Use callbacks and best practices  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Experiment with different architectures\n",
    "2. Try other datasets\n",
    "3. Learn about Convolutional Neural Networks (CNNs) for images\n",
    "4. Explore Recurrent Neural Networks (RNNs) for sequences\n",
    "5. Check out the PyTorch tutorial to compare frameworks\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Keras Documentation](https://keras.io/)\n",
    "- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)\n",
    "- [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n",
    "\n",
    "Keep learning and building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
